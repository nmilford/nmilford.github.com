
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Setting up Cloudera's Hadoop CDH2 Distribution on CentOS - blog.milford.io</title>
  <meta name="author" content="Nathan Milford">

  
  <meta name="description" content="From Hadoop’s homepage: Apache Hadoop is a framework for running applications on large clusters built of commodity hardware. The Hadoop framework &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://code.milford.io/2010/06/setting-up-clouderas-hadoop-cdh2-distribution-on-centos/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="blog.milford.io" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">blog.milford.io</a></h1>
  
    <h2>a personal knowledge dump.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:code.milford.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/cassandra-token-calculator">Cassandra Token Calculator</a></li>
  <li><a href="/about">About</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Setting Up Cloudera's Hadoop CDH2 Distribution on CentOS</h1>
    
    
      <p class="meta">
        








  


<time datetime="2010-06-20T22:05:11-04:00" pubdate data-updated="true">Jun 20<span>th</span>, 2010</time>
        
      </p>
    
  </header>


<div class="entry-content"><p><img class="center" src="/images/hadoop-logo.jpg"></p>

<p>From <a href="http://hadoop.apache.org/">Hadoop</a>’s homepage:</p>

<blockquote><p>Apache Hadoop is a framework for running applications on large clusters built of commodity hardware. The Hadoop framework transparently provides applications both reliability and data motion. Hadoop implements a computational paradigm named Map/Reduce, where the application is divided into many small fragments of work, each of which may be executed or reexecuted on any node in the cluster. In addition, it provides a distributed file system (HDFS) that stores data on the compute nodes, providing very high aggregate bandwidth across the cluster. Both Map/Reduce and the distributed file system are designed so that node failures are automatically handled by the framework.</p></blockquote>

<p>In short it’s a distributed batch processing mechanism that stores data across an array of nodes. Computing of that data is done on or near the node with the data and is reported back to a master. You can run other apps on top of that batch processing interface.</p>

<p>At Outbrain we are in the process moving our data warehouse over to a <a href="http://hadoop.apache.org/">Hadoop</a>/<a href="http://hadoop.apache.org/hive">Hive</a> setup and we currently use it in production to serve <a href="http://blog.outbrain.com/2010/05/new-reporting-system/">reports to our users</a>.</p>

<p>Hadoop provides the tools to:</p>

<ul>
<li><p>store data distributed over several nodes with a configurable level of redundancy</p></li>
<li><p>farm out that processing of that data to the nodes storing it by mapping the task into a bunch of smaller jobs then combing the results returned into a coherent result.</p></li>
</ul>


<p>Installing and setting up Hadoop isn’t too difficult, but there are a few intial pitfalls with the initial provided configuration files.</p>

<p>The real pain is tweaking it, which will be another article in the future if I ever get a handle on it all. There are so many knobs and dials to adjust globally and per job and so many things to take into account when creating a cluster it just depresses a border-line obsessive compulsive with control issues, like myself. The options are staggering and you can really mung up performance once you start to scale up. I am by no means a Hadoop specialist, but I can show you how to get it up and running.</p>

<p>Many of the other tutorials you might come across will guide you through setting up a standalone or psuedo-distributed config first. In this tutorial, I’m just dumping my notes from when I was kicking the tires on our cluster and adding some annotations.  Good luck, sucker!</p>

<!-- more -->


<p>First off, there are a few roles you need to know about and they fit into two categoties</p>

<p>HDFS (Hadoop Distributed File System) processes:</p>

<ul>
<li><strong>DataNode</strong> (DN) – Just what the name implies, a process that runs on each slave node on which you want to store data and manages the tasks of fetching and storing data locally.</li>
<li><strong>NameNode</strong> (NN) – Simply stated it keeps track of the data in the data nodes. It only runs on the master node.</li>
<li><strong>Secondary NameNode</strong> (SNN) – This process is unfortunately named. The name implies that it is there to take over in the event of the failure of the NN, but it is in fact not what it does. It should really be called the Assistant NameNode. Let’s expound below in a little aside:</li>
</ul>


<p><em>Secondary Name Node, WTF? An aside</em>:</p>

<p>While the DN and NN are easy to understand I feel I need to get a bit more verbose to justify/explain whats going on with the SNN.</p>

<p>Upon startup (and only then) the NN reads the state of the filesystem from its’ <code>fsimage</code> file then from that point on it appends file system changes to the <code>edits</code> file.  Think of the the <code>fsimage</code> as the initial state and the <code>edits</code> as the delta.  Again, only during startup, does the NN dirty it’s hands with the process of merging those <code>edits</code> into the <code>fsimage</code>.  If the cluster has been running for a long time and/or you have a sizable cluster this could take a while.</p>

<p>While the cluster is running, it is the SNN’s job to grab the <code>fsimage</code> and <code>edits</code> file from the NN, merges them into a new comprehensive <code>fsimage</code> and then uploads it back to the NN.</p>

<p>The prevailing wisdom is that the SNN should be on a seperate machine with the same ammount of RAM as the NN since it has similar needs.</p>

<ul>
<li><strong>Balancer</strong> – This isn’t really a full-time damon, but a process that is usually stared manually that redistrubutes your data evenely (variable by percentage) across your cluster. I usually put it in cron to run hourly.</li>
</ul>


<p>Mapred (Map/Reduce) processes.</p>

<ul>
<li><strong>Task Tracker</strong> (TT) – This process the runs locally on each slave and does all the computational heavy lifting.</li>
<li><strong>Job Tracker</strong> (JT) – This is the master process of the map/reduce system. The conductor of the symphony, if you will. It coordinates with the NN to know on which DN’s have the data to be processed and it splits up the job into smaller parts and ships it to TTs on or near those DNs. Once the TTs finish things up and report back, the JT puts it together gives you the result.
And those are the basic moving parts to the Hadoop ecosystem.</li>
</ul>


<p>We’ll be using the <a href="http://www.cloudera.com/">Cloudera</a> distribution, because it is easy to setup.</p>

<p>You also want a reasonably <a href="http://java.sun.com/javase/downloads/widget/jdk6.jsp">recent JVM</a> installed.</p>

<p>In general, I like to make sure the directory that Hadoop lives in is built using plenty of space, and the articles and advice I’ve read say that it performs better when you’re working with a JBOD rather than a RAID setup. I haven’t researched that, but I’ll assume for the moment they are smarter than me and I’ll take a bunch of disks and put them in the same LVM Volume group and span my logical volume across it. However, I don’t run a DN on my master and want the saftey of a RAID1 mirror for the fsimage and edits files</p>

<p>I also like to use <a href="http://xfs.org/">XFS</a>. In my experience it performs really well in the types of conditions an HDFS cluster works under.</p>

<p>So, for the master node I set it up thusly:</p>

<p>Assuming I have a bunch of disks in the ‘hadoop’ volume group, I create and format the hadoop logical volume.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>lvcreate -L 500G -n hadoop hadoop
</span><span class='line'>mkfs.xfs /dev/hadoop/hadoop</span></code></pre></td></tr></table></div></figure>


<p>Create the mount point.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mkdir -p /var/lib/hadoop-0.20</span></code></pre></td></tr></table></div></figure>


<p>I add the hadoop volume to the fstab and mount it.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo "/dev/hadoop/hadoop /var/lib/hadoop-0.20 xfs defaults 1 2" &gt;&gt; /etc/fstab
</span><span class='line'>mount -av</span></code></pre></td></tr></table></div></figure>


<p>Also, lets make some paths we’re gonna need.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mkdir /var/lib/hadoop-0.20/mapred/
</span><span class='line'>mkdir /var/lib/hadoop-0.20/mapred/local/
</span><span class='line'>mkdir /var/lib/hadoop-0.20/mapred/temp/
</span><span class='line'>mkdir /var/lib/hadoop-0.20/dfs/
</span><span class='line'>mkdir /var/lib/hadoop-0.20/logs/</span></code></pre></td></tr></table></div></figure>


<p>Now we add the Cloudera testing yum repository and update yum.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>wget -O /etc/yum.repos.d/cloudera-testing.repo \
</span><span class='line'>http://archive.cloudera.com/redhat/cdh/cloudera-testing.repo
</span><span class='line'>yum update yum</span></code></pre></td></tr></table></div></figure>


<p>For my master node.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum -y install hadoop-0.20 hadoop-0.20-conf-pseudo-desktop \
</span><span class='line'>hadoop-0.20-docs hadoop-0.20-jobtracker hadoop-0.20-libhdfs \
</span><span class='line'>hadoop-0.20-namenode hadoop-0.20-native hadoop-0.20-pipes</span></code></pre></td></tr></table></div></figure>


<p>Note that I omitted the SNN, DN &amp; TT packages as well as installed the Cloudera Desktop conf package. Cloudera desktop is a pretty sweet add-on that gives you a lot of great insight into whats occuring on your cluster at any point in time.</p>

<p>I pick a second node to act as my SNN and install these.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum -y install hadoop-0.20 hadoop-0.20-conf-pseudo-desktop \
</span><span class='line'>hadoop-0.20-datanode hadoop-0.20-tasktracker hadoop-0.20-secondarynamenode</span></code></pre></td></tr></table></div></figure>


<p>And on the normal slaves I just install these.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum -y install hadoop-0.20 hadoop-0.20-conf-pseudo-desktop \
</span><span class='line'>hadoop-0.20-datanode hadoop-0.20-tasktracker</span></code></pre></td></tr></table></div></figure>


<p>Now let’s start getting our config on.</p>

<p>Everything is either in <em>-default.xml with exceptions in </em>-site.xml The idea is that -default is read only and -site overrides the former. Jobs can be submitted requesting overrides to as well.</p>

<p>PRO TIP: Adding <code>&lt;final&gt;true&lt;/final&gt;</code> to the directive will make it so downstream configs cannot override it.</p>

<p>The original psuedo config presumes that you’re running everything on a single server so it throws everything dynamically in /tmp which will not suite us at all, so we need to explicitly define some junk.</p>

<p>I also found myself confused at times as to which config items were important or not and what paths were on the local filesystem or in HDFS.</p>

<p>Here is my basic <code>/etc/hadoop-0.20/conf/hdfs-site.xml</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;configuration&gt;
</span><span class='line'>  &lt;!-- Enable Cloudera Desktop Plugins --&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;dfs.namenode.plugins&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;org.apache.hadoop.thriftfs.NamenodePlugin&lt;/value&gt;
</span><span class='line'>    &lt;description&gt;Comma-separated list of namenode plug-ins to be activated.
</span><span class='line'>    &lt;/description&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;dfs.datanode.plugins&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;org.apache.hadoop.thriftfs.DatanodePlugin&lt;/value&gt;
</span><span class='line'>    &lt;description&gt;Comma-separated list of datanode plug-ins to be activated.
</span><span class='line'>    &lt;/description&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;dfs.thrift.address&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;0.0.0.0:9090&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;  
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;dfs.name.dir&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;/var/lib/hadoop-0.20/dfs/name,/var/lib/hadoop-0.20/dfs_mr2/name&lt;/value&gt;
</span><span class='line'>    &lt;final&gt;true&lt;/final&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;fs.checkpoint.dir&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;/var/lib/hadoop-0.20/dfs/namesecondary&lt;/value&gt;
</span><span class='line'>    &lt;final&gt;true&lt;/final&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;dfs.data.dir&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;/var/lib/hadoop-0.20/dfs/data&lt;/value&gt;
</span><span class='line'>    &lt;final&gt;true&lt;/final&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;4096&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;dfs.replication&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;2&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>     &lt;name&gt;dfs.permissions&lt;/name&gt;
</span><span class='line'>     &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>&lt;/configuration&gt;</span></code></pre></td></tr></table></div></figure>


<p>You’ll note that I have two paths seperated by a comma for <code>dfs.name.dir</code>. This is a pretty nice feature that allows you to write the NN data to multiple targets. In this case, I have an NFS mount setup on the host mr2 (my SNN) so I can write a copy of the NN metadata off-machine for safety.</p>

<p>Here is my basic <code>/etc/hadoop-0.20/conf/mapred-site.xml</code>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;?xml version="1.0"?&gt;
</span><span class='line'>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>&lt;configuration&gt;
</span><span class='line'>  &lt;!-- Enable Cloudera Desktop plugins --&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;mapred.jobtracker.plugins&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;org.apache.hadoop.thriftfs.ThriftJobTrackerPlugin&lt;/value&gt;
</span><span class='line'>    &lt;description&gt;Comma-separated list of jobtracker plug-ins to be activated.
</span><span class='line'>    &lt;/description&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>     &lt;name&gt;mapred.job.tracker&lt;/name&gt;
</span><span class='line'>     &lt;value&gt;jobtracker.yourcompany.com:8021&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>     &lt;name&gt;mapred.system.dir&lt;/name&gt;
</span><span class='line'>     &lt;value&gt;/var/lib/hadoop-0.20/mapred/system&lt;/value&gt;
</span><span class='line'>     &lt;final&gt;true&lt;/final&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>     &lt;name&gt;mapred.local.dir&lt;/name&gt;
</span><span class='line'>     &lt;value&gt;/var/lib/hadoop-0.20/mapred/local&lt;/value&gt;
</span><span class='line'>     &lt;final&gt;true&lt;/final&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>     &lt;name&gt;mapred.temp.dir&lt;/name&gt;
</span><span class='line'>     &lt;value&gt;/var/lib/hadoop-0.20/mapred/temp&lt;/value&gt;
</span><span class='line'>     &lt;final&gt;true&lt;/final&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>     &lt;name&gt;mapred.tasktracker.map.tasks.maximum&lt;/name&gt;
</span><span class='line'>     &lt;value&gt;4&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>     &lt;name&gt;mapred.tasktracker.reduce.tasks.maximum&lt;/name&gt;
</span><span class='line'>     &lt;value&gt;4&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>     &lt;name&gt;mapred.userlog.retain.hours&lt;/name&gt;
</span><span class='line'>     &lt;value&gt;8&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>&lt;/configuration&gt;</span></code></pre></td></tr></table></div></figure>


<p>I have some tweaks there to define the number of map and reduce jobs that can be run as well as userlog retention timeframe (they can grow out of control).</p>

<p>Let’s add the masters.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo "mr1.yourcompany.com" &gt; /etc/hadoop-0.20/conf/masters
</span><span class='line'>echo "mr2.yourcompany.com" &gt; /etc/hadoop-0.20/conf/masters</span></code></pre></td></tr></table></div></figure>


<p>In this case I added the SNN in there too. The script that starts the cluster will attempt to fire up all of the master processes on it, but since we installed the SNN only on this node, it’ll only be successful getting that process going.</p>

<p>Let’s add the slaves.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo "mr2.nydc1.outbrain.com" &gt;&gt; /etc/hadoop-0.20/conf/slaves
</span><span class='line'>echo "mr3.nydc1.outbrain.com" &gt;&gt; /etc/hadoop-0.20/conf/slaves</span></code></pre></td></tr></table></div></figure>


<p>and so on&#8230;</p>

<p>Setup a more spacious logging target otherwise <code>/var/log</code> will get eaten up.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo "export HADOOP_LOG_DIR=/var/lib/hadoop-0.20/logs" &gt;&gt; /etc/hadoop-0.20/conf/hadoop-env.sh</span></code></pre></td></tr></table></div></figure>


<p>Now to modify the hadoop user setup by the RPMs to allow passwordless logins (for starting up the daemons..).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>usermod -d /var/lib/hadoop-0.20 -s /bin/bash hadoop
</span><span class='line'>su - hadoop
</span><span class='line'>ssh-keygen -t rsa -P ""
</span><span class='line'>cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys
</span><span class='line'>chmod 600 /var/lib/hadoop-0.20/.ssh/id_rsa
</span><span class='line'>chmod 644 /var/lib/hadoop-0.20/.ssh/id_rsa.pub
</span><span class='line'>chmod 644 /var/lib/hadoop-0.20/.ssh/authorized_keys
</span><span class='line'>chown -R hadoop:hadoop /var/lib/hadoop-0.20</span></code></pre></td></tr></table></div></figure>


<p>On the slaves.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>usermod -d /var/lib/hadoop-0.20 -s /bin/bash hadoop
</span><span class='line'>su - hadoop
</span><span class='line'>mkdir ~/.ssh
</span><span class='line'>chmod 700 .ssh/
</span><span class='line'>Put your pub key from the master in $HOME/.ssh/authorized_keys
</span><span class='line'>chmod 644 /var/lib/hadoop-0.20/.ssh/authorized_keys
</span><span class='line'>chown -R hadoop:hadoop /var/lib/hadoop-0.20</span></code></pre></td></tr></table></div></figure>


<p>Make it so hadoop is consistantly in the path.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo "export HADOOP_HOME="/usr/lib/hadoop-0.20/bin/" &gt; /etc/profile.d/hadoop.sh
</span><span class='line'>echo "PATH=${HADOOP_HOME}/bin:${PATH}" &gt; &gt; /etc/profile.d/hadoop.sh</span></code></pre></td></tr></table></div></figure>


<p>From the master, you ought to be able to start everything up by simply issuing this command.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>start-all.sh</span></code></pre></td></tr></table></div></figure>


<p>If you have trouble starting everything up, and the NN is spitting out errors, you may need to format it. The RPMs should do it for you upon install, but you may end up changing your <code>dfs.name.dir</code> which mean’s you’ll need to manually format the NN like this.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>stop-all.sh
</span><span class='line'>hadoop namenode -format</span></code></pre></td></tr></table></div></figure>


<p>This ought to get you going. You’ll certainly have problems here and there, but this ought to get you through the big missteps I took.</p>

<p>Later on I’ll write about setting up Hive.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Nathan Milford</span></span>

      








  


<time datetime="2010-06-20T22:05:11-04:00" pubdate data-updated="true">Jun 20<span>th</span>, 2010</time>
      


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://code.milford.io/2010/06/setting-up-clouderas-hadoop-cdh2-distribution-on-centos/" data-via="NathanMilford" data-counturl="http://code.milford.io/2010/06/setting-up-clouderas-hadoop-cdh2-distribution-on-centos/" >Tweet</a>
  
  
  
</div>




<div class="OUTBRAIN" data-src="http://code.milford.io/2010/06/setting-up-clouderas-hadoop-cdh2-distribution-on-centos/"></div>
<script type="text/javascript" async="async" src="http://widgets.outbrain.com/outbrain.js"></script> 



    
    <p class="meta">
      
        <a class="basic-alignment left" href="/2010/06/alt-installing-python-2-6-from-source-in-centos/" title="Previous Post: Alt-Installing Python 2.6 from source in CentOS">&laquo; Alt-Installing Python 2.6 from source in CentOS</a>
      
      
        <a class="basic-alignment right" href="/2010/06/in-the-hackers-bazaar-my-favorite-short-story/" title="Next Post: In the Hacker's Bazaar - My Favorite Short Story.">In the Hacker's Bazaar - My Favorite Short Story. &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/2012/03/getting-a-basic-cobbler-server-going-on-centos/">Getting a Basic Cobbler server going on CentOS.</a>
      </li>
    
      <li class="post">
        <a href="/2012/03/making-a-local-centos-mirror/">Making a local CentOS mirror.</a>
      </li>
    
      <li class="post">
        <a href="/2012/03/making-a-simple-yum-repository/">Making a simple Yum repository.</a>
      </li>
    
      <li class="post">
        <a href="/2012/03/now-hiring-now-right-now-were-hiring-now/">Now Hiring Now, Right Now We're Hiring Now.</a>
      </li>
    
      <li class="post">
        <a href="/2012/03/the-dell-c6220-and-an-ode-to-my-dell-dcs-team/">The Dell C6220 and an Ode to my Dell DCS Team.</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Tag Cloud</h1>
    <span id="tag-cloud"></span>
</section>

<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("NathanMilford", 4, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/NathanMilford" class="twitter-follow-button" data-show-count="false">Follow @NathanMilford</a>
  
</section>


<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/nmilford">@nmilford</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'nmilford',
            count: 6,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


<section>
  <h1>Coderwall</h1>
  <ul id="cw_badges">
    <li class="loading">Status updating...</li>
  </ul>

  <script type="text/javascript">
    var coderwall = (function(){
      function render(options, badges){
        var fragment = '',
            t = $(options.target)[0],
            height = 65,
            width = 65,
            index;

        for (index in badges) {
          fragment += '<a class="cw_badge"title="' + badges[index].description + '" href="http://coderwall.com/' + options.user + '">';
          fragment +=   '<img alt="' + badges[index].description + '" height="' + width + '" width="' + height + '" src="' + badges[index].badge + '"/>';
          fragment += '</a>';
        }

        t.innerHTML = fragment;
      }
      return {
        showBadges: function(options){
          $.ajax({
              url: 'http://coderwall.com/' + options.user + '.json?callback=?'
            , type: 'jsonp'
            , error: function (err) { $(options.target + ' li.loading').addClass('error').text("Error loading feed"); }
            , success: function(res) {
                render(options, res.data.badges);
            }
          });
        }
      };
    })();

    $.domReady(function(){
      if (!window.jXHR){
        var jxhr = document.createElement('script');
        jxhr.type = 'text/javascript';
        jxhr.src = '/javascripts/libs/jXHR.js';
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(jxhr, s);
      }
      if (!window.$){
        var b = document.createElement('script');
        b.type = 'text/javascript';
        b.src = '/javascripts/ender.js';
        var sc = document.getElementsByTagName('script')[0];
        sc.parentNode.insertBefore(jxhr, s);
      }
      coderwall.showBadges({
        user: 'nmilford',
        target: '#cw_badges'
      });
    });
  </script>
  <style type="text/css">
    .cw_badge img {
      padding: 5px;
      border: 0 none !important;
      -moz-box-shadow: none !important;
      -webkit-box-shadow: none !important;
      -o-box-shadow: none !important;
      box-shadow: none !important;
    }
  </style>
</section>


  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - Nathan Milford -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>


<input type="hidden" name="OBKey" value=''/> 
<script LANGUAGE="JavaScript">
var OBCTm='';
</script>
<script LANGUAGE="JavaScript" src="http://widgets.outbrain.com/claim.js"></script>


</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'code-milford-io';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://code.milford.io/2010/06/setting-up-clouderas-hadoop-cdh2-distribution-on-centos/';
        var disqus_url = 'http://code.milford.io/2010/06/setting-up-clouderas-hadoop-cdh2-distribution-on-centos/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>









  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
